{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c0d5f3a-96ad-43a3-98bd-f33989f952e4",
   "metadata": {},
   "source": [
    "# Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b320fac4-bc6f-44e6-a76a-28cba31189aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#!pip install data_loader\n",
    "import utils\n",
    "import config\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "#%cd Transformer-Pytorch\n",
    "from data_loader import MTDataset\n",
    "from model import make_model , LabelSmoothing\n",
    "from train import MultiGPULossCompute\n",
    "from utils import france_tokenizer_load\n",
    "from beam_decoder import beam_search\n",
    "from model import batch_greedy_decode\n",
    "import sacrebleu\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2568cb-f0b5-46a4-812e-ea431e2aab59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52417206-d0a6-490a-91b3-d9bd4149f660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines of france:  112000\n",
      "lines of English:  112000\n",
      " -------- Get Corpus ! -------- \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "files = ['train', 'dev', 'test']\n",
    "fr_path ='./data/corpus.fr'\n",
    "en_path ='./data/corpus.en'\n",
    "fr_lines = []\n",
    "en_lines = []\n",
    "\n",
    "for file in files:\n",
    "    corpus = json.load(open('./data/json/' + file +'.json', 'r', encoding=\"utf8\"))\n",
    "    for item in corpus.values():\n",
    "        fr_lines.append(item[1]+'\\n')\n",
    "        en_lines.append(item[0] + '\\n')\n",
    "\n",
    "with open(fr_path, \"w\", encoding=\"utf8\") as fch:\n",
    "    fch.writelines(fr_lines)\n",
    "\n",
    "with open(en_path, \"w\", encoding=\"utf8\") as fen:\n",
    "    fen.writelines(en_lines)\n",
    "\n",
    "print(\"lines of france: \", len(fr_lines))\n",
    "\n",
    "print(\"lines of English: \", len(en_lines))\n",
    "print(\" -------- Get Corpus ! -------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c4bd798-2dfd-4d4a-8758-5c498adc027d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Hello', ',', '▁how', '▁are', '▁you', '?']\n",
      "['▁Bonjour', ',', '▁comment', '▁allez', '-', 'vous', '?']\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "def train(input_file, vocab_size, model_name, model_type, character_coverage):\n",
    "  \n",
    "    input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --model_type=%s --character_coverage=%s ' \\\n",
    "                     '--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3' % (\n",
    "                         input_file, model_name, vocab_size, model_type, character_coverage\n",
    "                     )\n",
    "    spm.SentencePieceTrainer.Train(input_argument)\n",
    "\n",
    "\n",
    "def piece_run():\n",
    "    # ===== English =====\n",
    "    en_input = './data/corpus.en'  # raw input corpus file\n",
    "    en_vocab_size = 32000\n",
    "    en_model_name = './tokenizer/eng'\n",
    "    en_model_type = 'bpe'\n",
    "    en_character_coverage = 1.0\n",
    "    train(en_input, en_vocab_size, en_model_name, en_model_type, en_character_coverage)\n",
    "\n",
    "    # ===== French =====\n",
    "    fr_input = './data/corpus.fr'\n",
    "    fr_vocab_size = 32000\n",
    "    fr_model_name = './tokenizer/fra'\n",
    "    fr_model_type = 'bpe'\n",
    "    fr_character_coverage = 0.9995\n",
    "    train(fr_input, fr_vocab_size, fr_model_name, fr_model_type, fr_character_coverage)\n",
    "\n",
    "\n",
    "# Run the tokenizer training\n",
    "#piece_run()\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Load the existing models\n",
    "sp_en = spm.SentencePieceProcessor()\n",
    "sp_en.Load('./tokenizer/eng.model')\n",
    "\n",
    "sp_fr = spm.SentencePieceProcessor()\n",
    "sp_fr.Load('./tokenizer/fra.model')\n",
    "\n",
    "# Test\n",
    "print(sp_en.EncodeAsPieces(\"Hello, how are you?\"))\n",
    "print(sp_fr.EncodeAsPieces(\"Bonjour, comment allez-vous?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e91c1c52-6759-45d3-87e3-807f1af5565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip -d ./tokenizer ./tokenizer/token.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "077f20e9-a01e-4872-a437-3ea3e0a4c4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU quantity: 1\n",
      "Whether GPU is available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU quantity:\", torch.cuda.device_count())\n",
    "print(\"Whether GPU is available:\", torch.cuda.is_available())\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ef92863-5a3c-4786-a90d-2a28bf9cb3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Model dimensions\n",
    "d_model=512\n",
    "n_heads=8\n",
    "n_layers=6\n",
    "d_k=64\n",
    "d_v=64\n",
    "d_ff=2048\n",
    "dropout=0.1\n",
    "\n",
    "# Special label index\n",
    "padding_idx=0\n",
    "bos_idx=2\n",
    "eos_idx=3\n",
    "\n",
    "#Vocabulary size\n",
    "src_vocab_size=32000\n",
    "tgt_vocab_size=32000\n",
    "#Training parameters\n",
    "batch_size=32\n",
    "epoch_num=2\n",
    "early_stop=5\n",
    "lr=3e-5\n",
    "\n",
    "#Decoding partameters\n",
    "max_len=60\n",
    "beam_size=3\n",
    "\n",
    "#other options\n",
    "use_smoothing=False\n",
    "use_naomopt=True\n",
    "# Data model path\n",
    "data_dir='./data'\n",
    "train_data_path='./data/json/train.json'\n",
    "dev_data_path='./data/json/dev.json'\n",
    "test_data_path='./data/json/test.json'\n",
    "model_path='./experiment/model.pth'\n",
    "pretrain_model_path='./pretrain/model.pth'\n",
    "log_path='./experiment/train.log'\n",
    "output_path='./experiment/output.txt'\n",
    "\n",
    "# GPU and device setting\n",
    "gpu_id=''\n",
    "device_id=[0]\n",
    "#set device\n",
    "if gpu_id!='':\n",
    "    device=torch.device(f\"cuda:{gpu_id}\")\n",
    "else:\n",
    "    device=torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c48f0af-e756-46e4-92d5-3dce506a634c",
   "metadata": {},
   "source": [
    "### Obtain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a57fb5ba-374c-43c8-b4cb-0ce7a777d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import english_tokenizer_load, france_tokenizer_load\n",
    "\n",
    "from utils import english_tokenizer_load, france_tokenizer_load\n",
    "import config\n",
    "\n",
    "DEVICE = config.device\n",
    "\n",
    "\n",
    "# Generate subsequent positional mask (for decoder)\n",
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype(\"uint8\")\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    def __init__(self, src, trg=None, pad=0, src_text=None, trg_text=None):\n",
    "        self.src_text = src_text\n",
    "        self.trg_text = trg_text\n",
    "        self.src = src.to(DEVICE)\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1].to(DEVICE)\n",
    "            self.trg_y = trg[:, 1:].to(DEVICE)\n",
    "            self.trg_mask = self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(trg, pad):\n",
    "        trg_mask = (trg != pad).unsqueeze(-2)\n",
    "        trg_mask = trg_mask & subsequent_mask(trg.size(-1)).type_as(trg_mask.data)\n",
    "        return trg_mask\n",
    "\n",
    "\n",
    "class MTDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        # Load English and French sentences\n",
    "        self.out_en_sent, self.out_fr_sent = self.get_dataset(data_path, sort=True)\n",
    "\n",
    "        # Load tokenizers\n",
    "        self.sp_eng = english_tokenizer_load()\n",
    "        self.sp_fra = france_tokenizer_load()\n",
    "\n",
    "        # Special tokens\n",
    "        self.PAD = self.sp_eng.pad_id()\n",
    "        self.BOS = self.sp_eng.bos_id()\n",
    "        self.EOS = self.sp_eng.eos_id()\n",
    "\n",
    "    @staticmethod\n",
    "    def len_argsort(seq):\n",
    "        \"\"\"Sort indexes based on the sequence length.\"\"\"\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "    def get_dataset(self, data_path, sort=False):\n",
    "        dataset = json.load(open(data_path, 'r'))  # load the JSON dict\n",
    "        out_en_sent = []\n",
    "        out_fr_sent = []\n",
    "\n",
    "        # iterate over the dictionary keys in order\n",
    "        for key in sorted(dataset.keys(), key=int):\n",
    "            out_en_sent.append(dataset[key][0])  # English sentence\n",
    "            out_fr_sent.append(dataset[key][1])  # French sentence\n",
    "\n",
    "        if sort:\n",
    "            sorted_index = self.len_argsort(out_en_sent)\n",
    "            out_en_sent = [out_en_sent[i] for i in sorted_index]\n",
    "            out_fr_sent = [out_fr_sent[i] for i in sorted_index]\n",
    "\n",
    "        return out_en_sent, out_fr_sent\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eng_text = self.out_en_sent[idx]\n",
    "        fra_text = self.out_fr_sent[idx]\n",
    "        src = [self.BOS] + self.sp_eng.EncodeAsIds(eng_text) + [self.EOS]\n",
    "        trg = [self.BOS] + self.sp_fra.EncodeAsIds(fra_text) + [self.EOS]\n",
    "        return [src, trg, fra_text]  # include raw text\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of examples in the dataset\"\"\"\n",
    "        return len(self.out_en_sent)\n",
    "    def collate_fn(self, batch):\n",
    "        src_tokens = [torch.tensor(sample[0], dtype=torch.long) for sample in batch]\n",
    "        tgt_tokens = [torch.tensor(sample[1], dtype=torch.long) for sample in batch]\n",
    "        trg_text = [sample[2] for sample in batch]\n",
    "\n",
    "        src_tensor = pad_sequence(src_tokens, batch_first=True, padding_value=0)\n",
    "        tgt_tensor = pad_sequence(tgt_tokens, batch_first=True, padding_value=0)\n",
    "\n",
    "        #return Batch(src_tensor, tgt_tensor, trg_text=trg_text)\n",
    "        return Batch(src_tensor, tgt_tensor, pad=0, trg_text=trg_text)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8474e40-04b5-4e67-b745-be15824c4fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------- Dataset Build! -\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MTDataset(config.train_data_path)\n",
    "dev_dataset = MTDataset(config.dev_data_path)\n",
    "test_dataset = MTDataset(config.test_data_path)\n",
    "\n",
    "# logging.info(\"\n",
    "print(\" -------- Dataset Build! -\")\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=config.batch_size,\n",
    "collate_fn=train_dataset.collate_fn)\n",
    "dev_dataloader = DataLoader(dev_dataset, shuffle=False, batch_size=config.batch_size,\n",
    "collate_fn=dev_dataset.collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=config.batch_size,\n",
    "collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e325c5-780d-4fac-a8e9-3493d2c11fac",
   "metadata": {},
   "source": [
    "### Define the learning rate initialize model and define loss fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f94a3621-e894-4bd3-9b9d-a33231e4c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class NoamOpt:\n",
    "    \"\"\"Optim wrapper that implements the Noam learning rate schedule.\"\"\"\n",
    "\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Update parameters and rate\"\"\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate  # Corrected from 'Ir' to 'lr'\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        \"\"\"Compute learning rate at current step\"\"\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * (self.model_size ** (-0.5) *\n",
    "                              min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "\n",
    "\n",
    "def get_std_opt(model):\n",
    "    \"\"\"Standard Noam optimizer for a model\"\"\"\n",
    "    return NoamOpt(\n",
    "        model_size=model.src_embed[0].d_model,  # Assuming src_embed is a list or nn.ModuleList\n",
    "        factor=1,\n",
    "        warmup=10000,\n",
    "        optimizer=torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "    )\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = make_model(\n",
    "    config.src_vocab_size,\n",
    "    config.tgt_vocab_size,\n",
    "    config.n_layers,\n",
    "    config.d_model,\n",
    "    config.d_ff,\n",
    "    config.n_heads,\n",
    "    config.dropout\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_state_dict(torch.load(config.pretrain_model_path))\n",
    "\n",
    "# Wrap in DataParallel if multiple GPUs\n",
    "model_par = torch.nn.DataParallel(model)\n",
    "\n",
    "# Loss function and optimizer\n",
    "if config.use_smoothing:\n",
    "    criterion = LabelSmoothing(\n",
    "        size=config.tgt_vocab_size,\n",
    "        padding_idx=config.padding_idx,\n",
    "        smoothing=0.1\n",
    "    )\n",
    "    criterion.cuda()\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "\n",
    "if config.use_noamopt:\n",
    "    optimizer = get_std_opt(model)\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95aa714-7d06-43db-a378-8a8eb67d6df8",
   "metadata": {},
   "source": [
    "### Define training and verify the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5848655-76a1-4836-be22-ee28b7eda4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import sacrebleu\n",
    "\n",
    "def run_epoch(data, model, loss_compute):\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(data):\n",
    "        out = model(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "    \n",
    "    return total_loss / total_tokens\n",
    "\n",
    "\n",
    "def train(train_data, dev_data, model, model_par, criterion, optimizer):\n",
    "    \"\"\"Training loop with early stopping and BLEU evaluation.\"\"\"\n",
    "    best_bleu_score = 0.0\n",
    "    early_stop_counter = config.early_stop\n",
    "\n",
    "    for epoch in range(1, config.epoch_num + 1):\n",
    "        # ===== Training =====\n",
    "        model.train()\n",
    "        train_loss = run_epoch(\n",
    "            train_data, \n",
    "            model_par,\n",
    "            MultiGPULossCompute(model.generator, criterion, config.device_id, optimizer)\n",
    "        )\n",
    "        print(\"Epoch: {}, Train loss: {:.4f}\".format(epoch, train_loss))\n",
    "\n",
    "        # ===== Evaluation =====\n",
    "        model.eval()\n",
    "        dev_loss = run_epoch(\n",
    "            dev_data, \n",
    "            model_par,\n",
    "            MultiGPULossCompute(model.generator, criterion, config.device_id, None)\n",
    "        )\n",
    "\n",
    "        bleu_score = evaluate(dev_data, model)\n",
    "        print(\"Epoch: {}, Dev loss: {:.4f}, BLEU Score: {:.2f}\".format(epoch, dev_loss, bleu_score))\n",
    "\n",
    "        # ===== Checkpointing =====\n",
    "        if bleu_score > best_bleu_score:\n",
    "            torch.save(model.state_dict(), config.model_path)\n",
    "            best_bleu_score = bleu_score\n",
    "            early_stop_counter = config.early_stop\n",
    "            print(\" -------- Save Best Model! -------- \")\n",
    "        else:\n",
    "            early_stop_counter -= 1\n",
    "            print(\"Early Stop Left: {}\".format(early_stop_counter))\n",
    "            if early_stop_counter == 0:\n",
    "                print(\" -------- Early Stop! -------- \")\n",
    "                break\n",
    "\n",
    "\n",
    "def evaluate(data, model, mode='dev', use_beam=True):\n",
    "    sp_fra = france_tokenizer_load()\n",
    "    trg = []\n",
    "    res = []\n",
    "    first_batch = True  # only print once\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data):\n",
    "            fr_sent = batch.trg_text\n",
    "            src = batch.src\n",
    "            src_mask = (src != 0).unsqueeze(-2)\n",
    "            if use_beam:\n",
    "                decode_result, _ = beam_search(\n",
    "                    model, src, src_mask, config.max_len,\n",
    "                    config.padding_idx, config.bos_idx, config.eos_idx,\n",
    "                    config.beam_size, config.device\n",
    "                )\n",
    "            else:\n",
    "                decode_result = batch_greedy_decode(\n",
    "                    model, src, src_mask, max_len=config.max_len\n",
    "                )\n",
    "\n",
    "            # ===== DEBUG: add these lines =====\n",
    "            if first_batch:\n",
    "                print(\"type(decode_result):\", type(decode_result))\n",
    "                print(\"len(decode_result):\", len(decode_result))\n",
    "                print(\"type(decode_result[0]):\", type(decode_result[0]))\n",
    "                print(\"decode_result[0]:\", decode_result[0][:5])  # first 5 elements\n",
    "                if isinstance(decode_result[0], list) and len(decode_result[0]) > 0:\n",
    "                    print(\"type(decode_result[0][0]):\", type(decode_result[0][0]))\n",
    "                    print(\"decode_result[0][0]:\", decode_result[0][0])\n",
    "                first_batch = False\n",
    "            # ===== END DEBUG =====\n",
    "\n",
    "            # Then try decoding based on what you see\n",
    "            translation = [sp_fra.decode_ids([int(x) for x in _s[0]]) for _s in decode_result]\n",
    "            trg.extend(fr_sent)\n",
    "            res.extend(translation)\n",
    "\n",
    "    bleu = sacrebleu.corpus_bleu(res, [trg])\n",
    "    return float(bleu.score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ff488e7-ee4f-4e20-8ee6-4afa6c79bf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Get Dataloader-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [03:54<00:00, 13.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 3.9138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:01<00:00, 41.81it/s]\n",
      "  2%|▏         | 1/63 [00:00<00:10,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(decode_result): <class 'list'>\n",
      "len(decode_result): 32\n",
      "type(decode_result[0]): <class 'list'>\n",
      "decode_result[0]: [[2986, 145, 3], [2179, 145, 3], [2986, 31951, 107]]\n",
      "type(decode_result[0][0]): <class 'list'>\n",
      "decode_result[0][0]: [2986, 145, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:14<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Dev loss: 1.9242, BLEU Score: 23.83\n",
      " -------- Save Best Model! -------- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [03:53<00:00, 13.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 1.9159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:01<00:00, 41.44it/s]\n",
      "  2%|▏         | 1/63 [00:00<00:11,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(decode_result): <class 'list'>\n",
      "len(decode_result): 32\n",
      "type(decode_result[0]): <class 'list'>\n",
      "decode_result[0]: [[2179, 145, 3], [2179, 1668, 145], [2179, 70, 145]]\n",
      "type(decode_result[0][0]): <class 'list'>\n",
      "decode_result[0][0]: [2179, 145, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:15<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Dev loss: 1.6383, BLEU Score: 29.58\n",
      " -------- Save Best Model! -------- \n"
     ]
    }
   ],
   "source": [
    "print(\"------------- Get Dataloader-------------\")\n",
    "train(train_dataloader, dev_dataloader, model, model_par,criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd5fea-2030-49cc-bf18-5f9dce4c63b2",
   "metadata": {},
   "source": [
    "### Test the model with the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09814669-f277-40df-b45d-7bf673a40910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:06<00:00, 49.69it/s]\n",
      "  0%|          | 1/313 [00:00<00:59,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(decode_result): <class 'list'>\n",
      "len(decode_result): 32\n",
      "type(decode_result[0]): <class 'list'>\n",
      "decode_result[0]: [[2179, 145, 3], [2179, 1668, 145], [2179, 70, 145]]\n",
      "type(decode_result[0][0]): <class 'list'>\n",
      "decode_result[0][0]: [2179, 145, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:29<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.4554940462112427, Bleu Score: 31.12699291913909\n"
     ]
    }
   ],
   "source": [
    "def test(data, model, criterion):\n",
    "    with torch.no_grad():\n",
    "    # Load the model.\n",
    "        model.load_state_dict(torch.load(config.model_path))\n",
    "        model_par =torch.nn.DataParallel(model)\n",
    "        model.eval()\n",
    "        \n",
    "        test_loss = run_epoch(data, model_par,\n",
    "        MultiGPULossCompute(model.generator, criterion, config.device_id, None))\n",
    "        bleu_score = evaluate(data, model, 'test')\n",
    "        \n",
    "        print('Test loss: {}, Bleu Score: {}'.format(test_loss, bleu_score))\n",
    "\n",
    "test(test_dataloader, model, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee738a40-a0e5-4de9-a87c-3bee0bc69c77",
   "metadata": {},
   "source": [
    "### Make inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca753097-887c-4965-b843-ba3eeae4e261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il croit cela peut le prouver.\n"
     ]
    }
   ],
   "source": [
    "from utils import english_tokenizer_load_inf\n",
    "from utils import france_tokenizer_load_inf\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def translate(src, model, use_beam=True):\n",
    "    \"\"\"Use the trained model to translate a single sentence and print the translation result.\"\"\"\n",
    "    sp_fra = france_tokenizer_load_inf()\n",
    "    with torch.no_grad():\n",
    "        model.load_state_dict(torch.load(config.model_path))\n",
    "        model.eval()\n",
    "        src_mask = (src != 0).unsqueeze(-2)  \n",
    "        if use_beam:\n",
    "            decode_result, _ = beam_search(\n",
    "                model, src, src_mask, config.max_len,\n",
    "                config.padding_idx, config.bos_idx, config.eos_idx,\n",
    "                config.beam_size, config.device\n",
    "            )\n",
    "            decode_result = [h[0] for h in decode_result]  \n",
    "        else:\n",
    "            decode_result = batch_greedy_decode(model, src, src_mask, max_len=config.max_len)\n",
    "        \n",
    "        translation = [sp_fra.decode_ids(_s) for _s in decode_result]\n",
    "        print(translation[0])\n",
    "\n",
    "\n",
    "def one_sentence_translate(sent, beam_search_flag=True):\n",
    "    \"\"\"Translate a single English sentence.\"\"\"\n",
    "    # Initialize the model\n",
    "    model = make_model(\n",
    "        config.src_vocab_size, config.tgt_vocab_size, config.n_layers,\n",
    "        config.d_model, config.d_ff, config.n_heads, config.dropout\n",
    "    )\n",
    "    BOS = english_tokenizer_load_inf().bos_id()  # typically 2\n",
    "    EOS = english_tokenizer_load_inf().eos_id()  # typically 3\n",
    "    src_tokens = [[BOS] + english_tokenizer_load_inf().EncodeAsIds(sent) + [EOS]]\n",
    "    batch_input = torch.LongTensor(np.array(src_tokens)).to(config.device)\n",
    "    translate(batch_input, model, use_beam=beam_search_flag)\n",
    "\n",
    "\n",
    "def translate_example():\n",
    "    \"\"\"Translation result example of a single sentence\"\"\"\n",
    "    sent = \"He believes that he can prove it\"\n",
    "    one_sentence_translate(sent, beam_search_flag=True)\n",
    "\n",
    "\n",
    "translate_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b8099-9de0-4038-9d36-cdaa041237e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e014896d-fc0f-4ec9-a19a-7925f1d31161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.8",
   "language": "python",
   "name": "pytorch-1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
