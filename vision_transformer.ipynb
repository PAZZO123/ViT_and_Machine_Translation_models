{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c399566-4f1e-464c-88ca-bf2e0e8d1f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple/\n",
      "Collecting download\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/download/0.3.5/download-0.3.5-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: requests in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from download) (2.27.1)\n",
      "Requirement already satisfied: tqdm in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from download) (4.64.1)\n",
      "Requirement already satisfied: six in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from download) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from requests->download) (3.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from requests->download) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from requests->download) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from requests->download) (2022.9.14)\n",
      "Installing collected packages: download\n",
      "Successfully installed download-0.3.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f09b948-93db-4f97-8fcc-192a06244d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-02-25 20:53:47--  https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/vit_imagenet_dataset.zip\n",
      "Resolving proxy.modelarts.com (proxy.modelarts.com)... 192.168.0.180\n",
      "Connecting to proxy.modelarts.com (proxy.modelarts.com)|192.168.0.180|:80... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 512895647 (489M) [application/zip]\n",
      "Saving to: ‘vit_imagenet_dataset.zip’\n",
      "\n",
      "vit_imagenet_datase 100%[===================>] 489.13M  33.5MB/s    in 16s     \n",
      "\n",
      "2026-02-25 20:54:03 (30.8 MB/s) - ‘vit_imagenet_dataset.zip’ saved [512895647/512895647]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from download import download\n",
    "#!wget -c https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/vit_imagenet_dataset.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd02eacb-2d6f-4b9e-b579-21c1a0d3689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip vit_imagenet_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fe87b8c-c6ed-4529-aa0f-d326334d944f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple/\n",
      "Collecting mindspore\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/mindspore/2.0.0/mindspore-2.0.0-cp37-cp37m-manylinux1_x86_64.whl (947.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m947.2/947.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from mindspore) (1.19.5)\n",
      "Collecting asttokens>=2.0.4\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/asttokens/2.4.1/asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: psutil>=5.6.1 in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from mindspore) (5.8.0)\n",
      "Requirement already satisfied: protobuf>=3.13.0 in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from mindspore) (3.20.1)\n",
      "Collecting scipy>=1.5.4\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/scipy/1.7.3/scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from mindspore) (9.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from mindspore) (21.3)\n",
      "Collecting astunparse>=1.6.3\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/astunparse/1.6.3/astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from asttokens>=2.0.4->mindspore) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from astunparse>=1.6.3->mindspore) (0.37.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from packaging>=20.0->mindspore) (3.0.9)\n",
      "Installing collected packages: scipy, astunparse, asttokens, mindspore\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.5.2\n",
      "    Uninstalling scipy-1.5.2:\n",
      "      Successfully uninstalled scipy-1.5.2\n",
      "Successfully installed asttokens-2.4.1 astunparse-1.6.3 mindspore-2.0.0 scipy-1.7.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mindspore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77cb4ae2-f321-4864-a725-21e66c517e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mindspore as ms\n",
    "from mindspore.dataset import ImageFolderDataset\n",
    "import mindspore.dataset.vision as transforms\n",
    "\n",
    "data_path='./dataset/'\n",
    "mean=[0.485*255, 0.456*255, 0.406*255]\n",
    "std=[0.299*255, 0.244*255, 0.225*255]\n",
    "\n",
    "dataset_train=ImageFolderDataset(os.path.join(data_path, 'train'), shuffle=True)\n",
    "trans_train=[\n",
    "    transforms.RandomCropDecodeResize(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.33)),\n",
    "    transforms.RandomHorizontalFlip(prob=0.5),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "    transform.HWC2CHW()\n",
    "    \n",
    "]\n",
    "dataset_train=dataset_train.map(operations=trans_train, input_columns=[\"image\"])\n",
    "dataset_train=dataset_train.batch(batch_size=16, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b485714b-afd7-48a9-bc52-ed4749c6a098",
   "metadata": {},
   "source": [
    "### Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc12ccb3-a5df-4315-b912-962cbb88dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import nn, ops\n",
    "\n",
    "\n",
    "class Attention(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 dim: int,\n",
    "                 num_heads: int = 8,\n",
    "                 keep_prob: float = 1.0,\n",
    "                 attention_keep_prob: float = 1.0):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = ms.Tensor(head_dim ** -0.5)\n",
    "\n",
    "        self.qkv = nn.Dense(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(p=1.0-attention_keep_prob)\n",
    "        self.out = nn.Dense(dim, dim)\n",
    "        self.out_drop = nn.Dropout(p=1.0-keep_prob)\n",
    "        self.attn_matmul_v = ops.BatchMatMul()\n",
    "        self.q_matmul_k = ops.BatchMatMul(transpose_b=True)\n",
    "        self.softmax = nn.Softmax(axis=-1)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Attention construct.\"\"\"\n",
    "        b, n, c = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = ops.reshape(qkv, (b, n, 3, self.num_heads, c // self.num_heads))\n",
    "        qkv = ops.transpose(qkv, (2, 0, 3, 1, 4))\n",
    "        q, k, v = ops.unstack(qkv, axis=0)\n",
    "        attn = self.q_matmul_k(q, k)\n",
    "        attn = ops.mul(attn, self.scale)\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.attn_drop(attn)\n",
    "        out = self.attn_matmul_v(attn, v)\n",
    "        out = ops.transpose(out, (0, 2, 1, 3))\n",
    "        out = ops.reshape(out, (b, n, c))\n",
    "        out = self.out(out)\n",
    "        out = self.out_drop(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69bed71-541e-4bd7-bafd-2ef2dde3ac2e",
   "metadata": {},
   "source": [
    "### Transform Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54208aa2-4811-483e-a281-30bc2438a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict\n",
    "\n",
    "\n",
    "class FeedForward(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 hidden_features: Optional[int] = None,\n",
    "                 out_features: Optional[int] = None,\n",
    "                 activation: nn.Cell = nn.GELU,\n",
    "                 keep_prob: float = 1.0):\n",
    "        super(FeedForward, self).__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.dense1 = nn.Dense(in_features, hidden_features)\n",
    "        self.activation = activation()\n",
    "        self.dense2 = nn.Dense(hidden_features, out_features)\n",
    "        self.dropout = nn.Dropout(p=1.0-keep_prob)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Feed Forward construct.\"\"\"\n",
    "        x = self.dense1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualCell(nn.Cell):\n",
    "    def __init__(self, cell):\n",
    "        super(ResidualCell, self).__init__()\n",
    "        self.cell = cell\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"ResidualCell construct.\"\"\"\n",
    "        return self.cell(x) + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "430822ed-b032-4517-a832-072d47678d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 dim:int,\n",
    "                 num_layers:int,\n",
    "                 num_heads:int,\n",
    "                 mlp_dim:int,\n",
    "                 keep_prob:float=1.0,\n",
    "                 attention_keep_prob:float=1.0,\n",
    "                 drop_path_keep_prob:float=1.0,\n",
    "                 activation:nn.Cell=nn.GELU,\n",
    "                 norm:nn.Cell=nn.LayerNorm):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        layers=[]\n",
    "        for _ in range(num_layers):\n",
    "            normalization1=norm((dim,))\n",
    "            normalization2=norm((dim,))\n",
    "            attention=Attention(dim=dim,\n",
    "                                num_heads=num_heads,\n",
    "                                keep_prob=keep_prob,\n",
    "                                attention_keep_prob=attention_keep_prob)\n",
    "            \n",
    "            feedforward=FeedForward(in_features=dim,\n",
    "                                    hidden_features=mlp_dim,\n",
    "                                    activation=activation,\n",
    "                                    keep_prob=keep_prob)\n",
    "            \n",
    "            layers.append(\n",
    "            nn.SequentialCell([\n",
    "                ResidualCell(nn.SequentialCell([normalization1,attention])),\n",
    "                ResidualCell(nn.SequentialCell([normalization2,feedforward]))\n",
    "                \n",
    "            ])\n",
    "            )\n",
    "        self.layers=nn.SequentialCell(layers)\n",
    "    def construct(self, x):\n",
    "        return self.layers(x)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe7e17f-39d7-4787-9342-1958cdebfb79",
   "metadata": {},
   "source": [
    "### Input for ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df6f2c35-83b7-40a5-a9c7-1dfe9b688f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Cell):\n",
    "    MIN_NUM_PATCHES = 4\n",
    "\n",
    "    def __init__(self,\n",
    "                 image_size: int = 224,\n",
    "                 patch_size: int = 16,\n",
    "                 embed_dim: int = 768,\n",
    "                 input_channels: int = 3):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.conv = nn.Conv2d(input_channels, embed_dim, kernel_size=patch_size, stride=patch_size, has_bias=True)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Path Embedding construct.\"\"\"\n",
    "        x = self.conv(x)\n",
    "        b, c, h, w = x.shape\n",
    "        x = ops.reshape(x, (b, c, h * w))\n",
    "        x = ops.transpose(x, (0, 2, 1))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da22d6-d7f1-4342-821b-8fc309df4a05",
   "metadata": {},
   "source": [
    "#### building ViT As Whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6472ec84-c170-4b2d-8229-0604ff9d97a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.common.initializer import Normal\n",
    "from mindspore.common.initializer import initializer\n",
    "from mindspore import Parameter\n",
    "\n",
    "\n",
    "def init(init_type, shape, dtype, name, requires_grad):\n",
    "    \"\"\"Init.\"\"\"\n",
    "    initial = initializer(init_type, shape, dtype).init_data()\n",
    "    return Parameter(initial, name=name, requires_grad=requires_grad)\n",
    "\n",
    "\n",
    "class ViT(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 image_size: int = 224,\n",
    "                 input_channels: int = 3,\n",
    "                 patch_size: int = 16,\n",
    "                 embed_dim: int = 768,\n",
    "                 num_layers: int = 12,\n",
    "                 num_heads: int = 12,\n",
    "                 mlp_dim: int = 3072,\n",
    "                 keep_prob: float = 1.0,\n",
    "                 attention_keep_prob: float = 1.0,\n",
    "                 drop_path_keep_prob: float = 1.0,\n",
    "                 activation: nn.Cell = nn.GELU,\n",
    "                 norm: Optional[nn.Cell] = nn.LayerNorm,\n",
    "                 pool: str = 'cls') -> None:\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(image_size=image_size,\n",
    "                                              patch_size=patch_size,\n",
    "                                              embed_dim=embed_dim,\n",
    "                                              input_channels=input_channels)\n",
    "        num_patches = self.patch_embedding.num_patches\n",
    "\n",
    "        self.cls_token = init(init_type=Normal(sigma=1.0),\n",
    "                              shape=(1, 1, embed_dim),\n",
    "                              dtype=ms.float32,\n",
    "                              name='cls',\n",
    "                              requires_grad=True)\n",
    "\n",
    "        self.pos_embedding = init(init_type=Normal(sigma=1.0),\n",
    "                                  shape=(1, num_patches + 1, embed_dim),\n",
    "                                  dtype=ms.float32,\n",
    "                                  name='pos_embedding',\n",
    "                                  requires_grad=True)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.pos_dropout = nn.Dropout(p=1.0-keep_prob)\n",
    "        self.norm = norm((embed_dim,))\n",
    "        self.transformer = TransformerEncoder(dim=embed_dim,\n",
    "                                              num_layers=num_layers,\n",
    "                                              num_heads=num_heads,\n",
    "                                              mlp_dim=mlp_dim,\n",
    "                                              keep_prob=keep_prob,\n",
    "                                              attention_keep_prob=attention_keep_prob,\n",
    "                                              drop_path_keep_prob=drop_path_keep_prob,\n",
    "                                              activation=activation,\n",
    "                                              norm=norm)\n",
    "        self.dropout = nn.Dropout(p=1.0-keep_prob)\n",
    "        self.dense = nn.Dense(embed_dim, num_classes)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"ViT construct.\"\"\"\n",
    "        x = self.patch_embedding(x)\n",
    "        cls_tokens = ops.tile(self.cls_token.astype(x.dtype), (x.shape[0], 1, 1))\n",
    "        x = ops.concat((cls_tokens, x), axis=1)\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        x = self.pos_dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x)\n",
    "        x = x[:, 0]\n",
    "        if self.training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9911a9c9-414a-4221-b58a-d557dcaccb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://download.mindspore.cn/vision/classification/vit_b_16_224.ckpt (330.2 MB)\n",
      "\n",
      "file_sizes: 100%|████████████████████████████| 346M/346M [00:17<00:00, 20.1MB/s]\n",
      "Successfully downloaded file to ./ckpt/vit_b_16_224.ckpt\n",
      "epoch: 1 step: 125, loss is 1.7536486387252808\n",
      "Train epoch time: 1468055.026 ms, per step time: 11744.440 ms\n",
      "epoch: 2 step: 125, loss is 1.936659812927246\n",
      "Train epoch time: 1420450.987 ms, per step time: 11363.608 ms\n",
      "epoch: 3 step: 125, loss is 1.27670419216156\n",
      "Train epoch time: 1417112.150 ms, per step time: 11336.897 ms\n",
      "epoch: 4 step: 125, loss is 1.726880669593811\n",
      "Train epoch time: 1434416.142 ms, per step time: 11475.329 ms\n",
      "epoch: 5 step: 125, loss is 1.3512903451919556\n",
      "Train epoch time: 1442131.757 ms, per step time: 11537.054 ms\n",
      "epoch: 6 step: 125, loss is 1.1309744119644165\n",
      "Train epoch time: 1395188.117 ms, per step time: 11161.505 ms\n",
      "epoch: 7 step: 125, loss is 1.2316460609436035\n",
      "Train epoch time: 1418482.470 ms, per step time: 11347.860 ms\n",
      "epoch: 8 step: 125, loss is 1.1878708600997925\n",
      "Train epoch time: 1457489.839 ms, per step time: 11659.919 ms\n",
      "epoch: 9 step: 125, loss is 1.5979456901550293\n",
      "Train epoch time: 1412357.538 ms, per step time: 11298.860 ms\n",
      "epoch: 10 step: 125, loss is 1.310521125793457\n",
      "Train epoch time: 1408067.901 ms, per step time: 11264.543 ms\n"
     ]
    }
   ],
   "source": [
    "from mindspore.nn import LossBase\n",
    "from mindspore.train import LossMonitor, TimeMonitor, CheckpointConfig, ModelCheckpoint\n",
    "from mindspore import train\n",
    "\n",
    "# define super parameter\n",
    "epoch_size = 10\n",
    "momentum = 0.9\n",
    "num_classes = 1000\n",
    "resize = 224\n",
    "step_size = dataset_train.get_dataset_size()\n",
    "\n",
    "# construct model\n",
    "network = ViT()\n",
    "\n",
    "# load ckpt\n",
    "vit_url = \"https://download.mindspore.cn/vision/classification/vit_b_16_224.ckpt\"\n",
    "path = \"./ckpt/vit_b_16_224.ckpt\"\n",
    "\n",
    "vit_path = download(vit_url, path, replace=True)\n",
    "param_dict = ms.load_checkpoint(vit_path)\n",
    "ms.load_param_into_net(network, param_dict)\n",
    "\n",
    "# define learning rate\n",
    "lr = nn.cosine_decay_lr(min_lr=float(0),\n",
    "                        max_lr=0.00005,\n",
    "                        total_step=epoch_size * step_size,\n",
    "                        step_per_epoch=step_size,\n",
    "                        decay_epoch=10)\n",
    "\n",
    "# define optimizer\n",
    "network_opt = nn.Adam(network.trainable_params(), lr, momentum)\n",
    "\n",
    "\n",
    "# define loss function\n",
    "class CrossEntropySmooth(LossBase):\n",
    "    \"\"\"CrossEntropy.\"\"\"\n",
    "\n",
    "    def __init__(self, sparse=True, reduction='mean', smooth_factor=0., num_classes=1000):\n",
    "        super(CrossEntropySmooth, self).__init__()\n",
    "        self.onehot = ops.OneHot()\n",
    "        self.sparse = sparse\n",
    "        self.on_value = ms.Tensor(1.0 - smooth_factor, ms.float32)\n",
    "        self.off_value = ms.Tensor(1.0 * smooth_factor / (num_classes - 1), ms.float32)\n",
    "        self.ce = nn.SoftmaxCrossEntropyWithLogits(reduction=reduction)\n",
    "\n",
    "    def construct(self, logit, label):\n",
    "        if self.sparse:\n",
    "            label = self.onehot(label, ops.shape(logit)[1], self.on_value, self.off_value)\n",
    "        loss = self.ce(logit, label)\n",
    "        return loss\n",
    "\n",
    "\n",
    "network_loss = CrossEntropySmooth(sparse=True,\n",
    "                                  reduction=\"mean\",\n",
    "                                  smooth_factor=0.1,\n",
    "                                  num_classes=num_classes)\n",
    "\n",
    "# set checkpoint\n",
    "ckpt_config = CheckpointConfig(save_checkpoint_steps=step_size, keep_checkpoint_max=100)\n",
    "ckpt_callback = ModelCheckpoint(prefix='vit_b_16', directory='./ViT', config=ckpt_config)\n",
    "\n",
    "# initialize model\n",
    "# \"Ascend + mixed precision\" can improve performance\n",
    "ascend_target = (ms.get_context(\"device_target\") == \"Ascend\")\n",
    "if ascend_target:\n",
    "    model = train.Model(network, loss_fn=network_loss, optimizer=network_opt, metrics={\"acc\"}, amp_level=\"O2\")\n",
    "else:\n",
    "    model = train.Model(network, loss_fn=network_loss, optimizer=network_opt, metrics={\"acc\"}, amp_level=\"O0\")\n",
    "\n",
    "# train model\n",
    "model.train(epoch_size,\n",
    "            dataset_train,\n",
    "            callbacks=[ckpt_callback, LossMonitor(125), TimeMonitor(125)],\n",
    "            dataset_sink_mode=False,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d663496-ff1e-4a42-86f5-707a972b37f8",
   "metadata": {},
   "source": [
    "### Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20194e72-e42b-4e6c-b554-3e042d88e656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Top_1_Accuracy': 0.7635, 'Top_5_Accuracy': 0.94}\n"
     ]
    }
   ],
   "source": [
    "dataset_val = ImageFolderDataset(os.path.join(data_path, \"val\"), shuffle=True)\n",
    "\n",
    "trans_val = [\n",
    "    transforms.Decode(),\n",
    "    transforms.Resize(224 + 32),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "    transforms.HWC2CHW()\n",
    "]\n",
    "\n",
    "dataset_val = dataset_val.map(operations=trans_val, input_columns=[\"image\"])\n",
    "dataset_val = dataset_val.batch(batch_size=16, drop_remainder=True)\n",
    "\n",
    "\n",
    "network = ViT()\n",
    "\n",
    "\n",
    "param_dict = ms.load_checkpoint(vit_path)\n",
    "ms.load_param_into_net(network, param_dict)\n",
    "\n",
    "network_loss = CrossEntropySmooth(sparse=True,\n",
    "                                  reduction=\"mean\",\n",
    "                                  smooth_factor=0.1,\n",
    "                                  num_classes=num_classes)\n",
    "\n",
    "eval_metrics = {'Top_1_Accuracy': train.Top1CategoricalAccuracy(),\n",
    "                'Top_5_Accuracy': train.Top5CategoricalAccuracy()}\n",
    "\n",
    "if ascend_target:\n",
    "    model = train.Model(network, loss_fn=network_loss, optimizer=network_opt, metrics=eval_metrics, amp_level=\"O2\")\n",
    "else:\n",
    "    model = train.Model(network, loss_fn=network_loss, optimizer=network_opt, metrics=eval_metrics, amp_level=\"O0\")\n",
    "\n",
    "# evaluate model\n",
    "result = model.eval(dataset_val)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ee3c1f-3e96-49ad-a31c-a0efd882f948",
   "metadata": {},
   "source": [
    "### Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddaa65d7-c77f-4d39-9ae1-985e2761b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_infer = ImageFolderDataset(os.path.join(data_path, \"infer\"), shuffle=True)\n",
    "\n",
    "trans_infer = [\n",
    "    transforms.Decode(),\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "    transforms.HWC2CHW()\n",
    "]\n",
    "\n",
    "dataset_infer = dataset_infer.map(operations=trans_infer,\n",
    "                                  input_columns=[\"image\"],\n",
    "                                  num_parallel_workers=1)\n",
    "dataset_infer = dataset_infer.batch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26108e24-6eb9-40be-a3aa-e7d3c73734c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{236: 'Doberman'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from scipy import io\n",
    "\n",
    "\n",
    "class Color(Enum):\n",
    "    \n",
    "    red = (0, 0, 255)\n",
    "    green = (0, 255, 0)\n",
    "    blue = (255, 0, 0)\n",
    "    cyan = (255, 255, 0)\n",
    "    yellow = (0, 255, 255)\n",
    "    magenta = (255, 0, 255)\n",
    "    white = (255, 255, 255)\n",
    "    black = (0, 0, 0)\n",
    "\n",
    "\n",
    "def check_file_exist(file_name: str):\n",
    "    #check_file_exist.\n",
    "    if not os.path.isfile(file_name):\n",
    "        raise FileNotFoundError(f\"File `{file_name}` does not exist.\")\n",
    "\n",
    "\n",
    "def color_val(color):\n",
    "    \n",
    "    if isinstance(color, str):\n",
    "        return Color[color].value\n",
    "    if isinstance(color, Color):\n",
    "        return color.value\n",
    "    if isinstance(color, tuple):\n",
    "        assert len(color) == 3\n",
    "        for channel in color:\n",
    "            assert 0 <= channel <= 255\n",
    "        return color\n",
    "    if isinstance(color, int):\n",
    "        assert 0 <= color <= 255\n",
    "        return color, color, color\n",
    "    if isinstance(color, np.ndarray):\n",
    "        assert color.ndim == 1 and color.size == 3\n",
    "        assert np.all((color >= 0) & (color <= 255))\n",
    "        color = color.astype(np.uint8)\n",
    "        return tuple(color)\n",
    "    raise TypeError(f'Invalid type for color: {type(color)}')\n",
    "\n",
    "\n",
    "def imread(image, mode=None):\n",
    "    if isinstance(image, pathlib.Path):\n",
    "        image = str(image)\n",
    "\n",
    "    if isinstance(image, np.ndarray):\n",
    "        pass\n",
    "    elif isinstance(image, str):\n",
    "        check_file_exist(image)\n",
    "        image = Image.open(image)\n",
    "        if mode:\n",
    "            image = np.array(image.convert(mode))\n",
    "    else:\n",
    "        raise TypeError(\"Image must be a `ndarray`, `str` or Path object.\")\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def imwrite(image, image_path, auto_mkdir=True):\n",
    "    #imwrite.\n",
    "    if auto_mkdir:\n",
    "        dir_name = os.path.abspath(os.path.dirname(image_path))\n",
    "        if dir_name != '':\n",
    "            dir_name = os.path.expanduser(dir_name)\n",
    "            os.makedirs(dir_name, mode=777, exist_ok=True)\n",
    "\n",
    "    image = Image.fromarray(image)\n",
    "    image.save(image_path)\n",
    "\n",
    "\n",
    "def imshow(img, win_name='', wait_time=0):\n",
    "    #imshow\n",
    "    cv2.imshow(win_name, imread(img))\n",
    "    if wait_time == 0:  \n",
    "        while True:\n",
    "            ret = cv2.waitKey(1)\n",
    "\n",
    "            closed = cv2.getWindowProperty(win_name, cv2.WND_PROP_VISIBLE) < 1\n",
    "            \n",
    "            if closed or ret != -1:\n",
    "                break\n",
    "    else:\n",
    "        ret = cv2.waitKey(wait_time)\n",
    "\n",
    "\n",
    "def show_result(img: str,\n",
    "                result: Dict[int, float],\n",
    "                text_color: str = 'green',\n",
    "                font_scale: float = 0.5,\n",
    "                row_width: int = 20,\n",
    "                show: bool = False,\n",
    "                win_name: str = '',\n",
    "                wait_time: int = 0,\n",
    "                out_file: Optional[str] = None) -> None:\n",
    "    #Mark the prediction results on the picture\n",
    "    img = imread(img, mode=\"RGB\")\n",
    "    img = img.copy()\n",
    "    x, y = 0, row_width\n",
    "    text_color = color_val(text_color)\n",
    "    for k, v in result.items():\n",
    "        if isinstance(v, float):\n",
    "            v = f'{v:.2f}'\n",
    "        label_text = f'{k}: {v}'\n",
    "        cv2.putText(img, label_text, (x, y), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                    font_scale, text_color)\n",
    "        y += row_width\n",
    "    if out_file:\n",
    "        show = False\n",
    "        imwrite(img, out_file)\n",
    "\n",
    "    if show:\n",
    "        imshow(img, win_name, wait_time)\n",
    "\n",
    "\n",
    "def index2label():\n",
    "    \n",
    "    metafile = os.path.join(data_path, \"ILSVRC2012_devkit_t12/data/meta.mat\")\n",
    "    meta = io.loadmat(metafile, squeeze_me=True)['synsets']\n",
    "\n",
    "    nums_children = list(zip(*meta))[4]\n",
    "    meta = [meta[idx] for idx, num_children in enumerate(nums_children) if num_children == 0]\n",
    "\n",
    "    _, wnids, classes = list(zip(*meta))[:3]\n",
    "    clssname = [tuple(clss.split(', ')) for clss in classes]\n",
    "    wnid2class = {wnid: clss for wnid, clss in zip(wnids, clssname)}\n",
    "    wind2class_name = sorted(wnid2class.items(), key=lambda x: x[0])\n",
    "\n",
    "    mapping = {}\n",
    "    for index, (_, class_name) in enumerate(wind2class_name):\n",
    "        mapping[index] = class_name[0]\n",
    "    return mapping\n",
    "\n",
    "for i, image in enumerate(dataset_infer.create_dict_iterator(output_numpy=True)):\n",
    "    image = image[\"image\"]\n",
    "    image = ms.Tensor(image)\n",
    "    prob = model.predict(image)\n",
    "    label = np.argmax(prob.asnumpy(), axis=1)\n",
    "    mapping = index2label()\n",
    "    output = {int(label): mapping[int(label)]}\n",
    "    print(output)\n",
    "    show_result(img=\"./dataset/infer/n01440764/ILSVRC2012_test_00000279.JPEG\",\n",
    "                result=output,\n",
    "                out_file=\"./dataset/infer/ILSVRC2012_test_00000279.JPEG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566173f1-5e55-40ec-a610-a7382725bdfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.8",
   "language": "python",
   "name": "pytorch-1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
